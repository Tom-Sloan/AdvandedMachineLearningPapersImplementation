{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eac31063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Source: [1, 2, 3, 4, 1, 5]\n",
      "Encoded Target: [6, 7, 8, 0, 10, 11, 12, 13, 14]\n",
      "\n",
      "Predicted Token IDs: tensor([[ 3, 11,  3,  3,  1,  6,  1,  4, 11]])\n",
      "Predicted Sentence: sat a sat sat the it the on a\n",
      "Loss: 2.982117176055908\n",
      "Input: The dog jumped over the fence\n",
      "Translation: tensor([[ 0,  4,  0,  8, 11,  9, 12,  0, 11,  5,  8]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import string\n",
    "from gpt_example import create_transformer_model\n",
    "\n",
    "# =========================\n",
    "# 1. Encoding Utilities\n",
    "# =========================\n",
    "\n",
    "# Define word-to-integer mappings for encoder and decoder\n",
    "word_to_int = {\"the\": 1, \"cat\": 2, \"sat\": 3, \"on\": 4, \"mat\": 5,\n",
    "    \"it\": 6, \"sounds\": 7, \"like\": 8, \"you're\": 9, \"quoting\": 10,\n",
    "    \"a\": 11, \"classic\": 12, \"simple\": 13, \"sentence\": 14\n",
    "}\n",
    "int_to_word = {v: k for k, v in word_to_int.items()}\n",
    "\n",
    "# Function to preprocess sentences: lowercase and remove punctuation\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "    return sentence\n",
    "\n",
    "# Function to convert sentence to numbers using encoder vocabulary\n",
    "def sentence_to_numbers_encoder(sentence):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    return [word_to_int.get(word, 0) for word in sentence.split()]\n",
    "\n",
    "# Function to convert numbers to sentence using decoder vocabulary\n",
    "def numbers_to_sentence_decoder(numbers):\n",
    "    # Exclude padding index '0' if present\n",
    "    return \" \".join([int_to_word.get(num, \"\") for num in numbers if num != 0])\n",
    "\n",
    "# =========================\n",
    "# 2. Embedding Layers\n",
    "# =========================\n",
    "\n",
    "# Define embedding dimensions\n",
    "embedding_dim = 16\n",
    "\n",
    "# Calculate vocabulary sizes\n",
    "vocab_size_encoder = len(word_to_int)  # 5\n",
    "vocab_size_decoder = len(word_to_int)  # 9\n",
    "\n",
    "# Initialize embedding layers (+1 for padding index '0')\n",
    "embedding_encoder = nn.Embedding(vocab_size_encoder + 1, embedding_dim)  # Encoder: 0-5\n",
    "embedding_decoder = nn.Embedding(vocab_size_decoder + 1, embedding_dim)  # Decoder: 0-14\n",
    "\n",
    "# =========================\n",
    "# 3. Positional Encoding\n",
    "# =========================\n",
    "\n",
    "# Define maximum sequence length\n",
    "max_len = 10  # Adjust as needed\n",
    "\n",
    "# Create positional encoding matrix\n",
    "pe = torch.zeros(1, max_len, embedding_dim)\n",
    "position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
    "div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-math.log(10000.0) / embedding_dim))\n",
    "pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "# =========================\n",
    "# 4. Transformer Components\n",
    "# =========================\n",
    "\n",
    "# Define number of attention heads\n",
    "num_heads = 2\n",
    "head_dim = embedding_dim // num_heads\n",
    "assert embedding_dim % num_heads == 0, \"Embedding dimension must be divisible by number of heads.\"\n",
    "\n",
    "# Initialize linear layers for Query, Key, Value\n",
    "Q_linear = nn.Linear(embedding_dim, embedding_dim)\n",
    "K_linear = nn.Linear(embedding_dim, embedding_dim)\n",
    "V_linear = nn.Linear(embedding_dim, embedding_dim)\n",
    "\n",
    "# Initialize linear layer for output of multi-head attention\n",
    "out_linear = nn.Linear(embedding_dim, embedding_dim)\n",
    "\n",
    "# Initialize feed-forward network layers\n",
    "ff_linear1 = nn.Linear(embedding_dim, 64)\n",
    "ff_relu = nn.ReLU()\n",
    "ff_linear2 = nn.Linear(64, embedding_dim)\n",
    "ff_dropout = nn.Dropout(0.1)\n",
    "\n",
    "# Initialize layer normalization layers\n",
    "layer_norm1 = nn.LayerNorm(embedding_dim)\n",
    "layer_norm2 = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "# Initialize dropout layer\n",
    "dropout = nn.Dropout(0.1)\n",
    "\n",
    "# =========================\n",
    "# 5. Preparing Input and Output\n",
    "# =========================\n",
    "\n",
    "# Define input and output sentences\n",
    "encoder_sentence = \"The cat sat on the mat.\"\n",
    "decoder_sentence = \"It sounds like you're quoting a classic simple sentence!\"\n",
    "\n",
    "# Encode the sentences\n",
    "encoded_src = sentence_to_numbers_encoder(encoder_sentence)\n",
    "print(\"Encoded Source:\", encoded_src)\n",
    "encoded_tgt = sentence_to_numbers_encoder(decoder_sentence)\n",
    "print(\"Encoded Target:\", encoded_tgt)\n",
    "print()\n",
    "\n",
    "# Convert encoded sentences to tensors and add batch dimension\n",
    "src_tensor = torch.tensor(encoded_src).unsqueeze(0)  # Shape: (1, src_seq_length)\n",
    "tgt_tensor = torch.tensor(encoded_tgt).unsqueeze(0)  # Shape: (1, tgt_seq_length)\n",
    "\n",
    "# =========================\n",
    "# 6. Forward Pass\n",
    "# =========================\n",
    "\n",
    "# =========================\n",
    "# Encoder\n",
    "# =========================\n",
    "\n",
    "# Step 1: Embed the source sentence\n",
    "src_embeddings = embedding_encoder(src_tensor)  # Shape: (1, src_seq_length, embedding_dim)\n",
    "\n",
    "# Step 2: Add positional encoding\n",
    "src_embeddings = src_embeddings + pe[:, :src_embeddings.size(1), :]  # Shape: (1, src_seq_length, embedding_dim)\n",
    "\n",
    "# Step 3: Compute Q, K, V matrices\n",
    "Q = Q_linear(src_embeddings)  # Shape: (1, src_seq_length, embedding_dim)\n",
    "K = K_linear(src_embeddings)  # Shape: (1, src_seq_length, embedding_dim)\n",
    "V = V_linear(src_embeddings)  # Shape: (1, src_seq_length, embedding_dim)\n",
    "\n",
    "# Step 4: Split Q, K, V for multi-head attention\n",
    "def split_heads(x):\n",
    "    batch_size, seq_length, dim = x.size()\n",
    "    x = x.view(batch_size, seq_length, num_heads, head_dim)\n",
    "    return x.transpose(1, 2)  # Shape: (batch_size, num_heads, seq_length, head_dim)\n",
    "\n",
    "Q = split_heads(Q)\n",
    "K = split_heads(K)\n",
    "V = split_heads(V)\n",
    "\n",
    "# Step 5: Scaled Dot-Product Attention\n",
    "scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(head_dim)  # Shape: (1, num_heads, src_seq_length, src_seq_length)\n",
    "attn_weights = F.softmax(scores, dim=-1)  # Shape: (1, num_heads, src_seq_length, src_seq_length)\n",
    "attn_output = torch.matmul(attn_weights, V)  # Shape: (1, num_heads, src_seq_length, head_dim)\n",
    "\n",
    "# Step 6: Concatenate heads\n",
    "attn_output = attn_output.transpose(1, 2).contiguous().view(1, src_tensor.size(1), embedding_dim)  # Shape: (1, src_seq_length, embedding_dim)\n",
    "\n",
    "# Step 7: Apply output linear layer\n",
    "attn_output = out_linear(attn_output)  # Shape: (1, src_seq_length, embedding_dim)\n",
    "\n",
    "# Step 8: Add residual connection and apply layer normalization\n",
    "src_embeddings = layer_norm1(src_embeddings + attn_output)  # Shape: (1, src_seq_length, embedding_dim)\n",
    "\n",
    "# =========================\n",
    "# Decoder\n",
    "# =========================\n",
    "\n",
    "# Step 1: Embed the target sentence\n",
    "tgt_embeddings = embedding_decoder(tgt_tensor)  # Shape: (1, tgt_seq_length, embedding_dim)\n",
    "\n",
    "# Step 2: Add positional encoding\n",
    "tgt_embeddings = tgt_embeddings + pe[:, :tgt_embeddings.size(1), :]  # Shape: (1, tgt_seq_length, embedding_dim)\n",
    "\n",
    "# Step 3: Compute Q, K, V matrices for decoder self-attention\n",
    "Q_dec = Q_linear(tgt_embeddings)  # Shape: (1, tgt_seq_length, embedding_dim)\n",
    "K_dec = K_linear(tgt_embeddings)  # Shape: (1, tgt_seq_length, embedding_dim)\n",
    "V_dec = V_linear(tgt_embeddings)  # Shape: (1, tgt_seq_length, embedding_dim)\n",
    "\n",
    "# Step 4: Split Q_dec, K_dec, V_dec for multi-head attention\n",
    "Q_dec = split_heads(Q_dec)\n",
    "K_dec = split_heads(K_dec)\n",
    "V_dec = split_heads(V_dec)\n",
    "\n",
    "# Step 5: Create look-ahead mask\n",
    "tgt_seq_length = tgt_tensor.size(1)\n",
    "look_ahead_mask = torch.triu(torch.ones(tgt_seq_length, tgt_seq_length), diagonal=1).bool()\n",
    "look_ahead_mask = look_ahead_mask.unsqueeze(0).unsqueeze(1)  # Shape: (1, 1, tgt_seq_length, tgt_seq_length)\n",
    "\n",
    "# Step 6: Scaled Dot-Product Attention for decoder self-attention with look-ahead mask\n",
    "scores_dec = torch.matmul(Q_dec, K_dec.transpose(-2, -1)) / math.sqrt(head_dim)  # Shape: (1, num_heads, tgt_seq_length, tgt_seq_length)\n",
    "scores_dec = scores_dec.masked_fill(look_ahead_mask, float('-inf'))\n",
    "attn_weights_dec = F.softmax(scores_dec, dim=-1)  # Shape: (1, num_heads, tgt_seq_length, tgt_seq_length)\n",
    "attn_output_dec = torch.matmul(attn_weights_dec, V_dec)  # Shape: (1, num_heads, tgt_seq_length, head_dim)\n",
    "\n",
    "# Step 7: Concatenate heads\n",
    "attn_output_dec = attn_output_dec.transpose(1, 2).contiguous().view(1, tgt_tensor.size(1), embedding_dim)  # Shape: (1, tgt_seq_length, embedding_dim)\n",
    "\n",
    "# Step 8: Apply output linear layer\n",
    "attn_output_dec = out_linear(attn_output_dec)  # Shape: (1, tgt_seq_length, embedding_dim)\n",
    "\n",
    "# Step 9: Add residual connection and apply layer normalization\n",
    "tgt_embeddings = layer_norm1(tgt_embeddings + attn_output_dec)  # Shape: (1, tgt_seq_length, embedding_dim)\n",
    "\n",
    "# =========================\n",
    "# Encoder-Decoder Attention\n",
    "# =========================\n",
    "\n",
    "# Step 1: Compute Q from decoder embeddings and K, V from encoder embeddings\n",
    "Q_enc_dec = Q_linear(tgt_embeddings)  # Shape: (1, tgt_seq_length, embedding_dim)\n",
    "K_enc_dec = K_linear(src_embeddings)  # Shape: (1, src_seq_length, embedding_dim)\n",
    "V_enc_dec = V_linear(src_embeddings)  # Shape: (1, src_seq_length, embedding_dim)\n",
    "\n",
    "# Step 2: Split Q_enc_dec, K_enc_dec, V_enc_dec for multi-head attention\n",
    "Q_enc_dec = split_heads(Q_enc_dec)\n",
    "K_enc_dec = split_heads(K_enc_dec)\n",
    "V_enc_dec = split_heads(V_enc_dec)\n",
    "\n",
    "# Step 3: Scaled Dot-Product Attention for encoder-decoder attention\n",
    "scores_enc_dec = torch.matmul(Q_enc_dec, K_enc_dec.transpose(-2, -1)) / math.sqrt(head_dim)  # Shape: (1, num_heads, tgt_seq_length, src_seq_length)\n",
    "attn_weights_enc_dec = F.softmax(scores_enc_dec, dim=-1)  # Shape: (1, num_heads, tgt_seq_length, src_seq_length)\n",
    "attn_output_enc_dec = torch.matmul(attn_weights_enc_dec, V_enc_dec)  # Shape: (1, num_heads, tgt_seq_length, head_dim)\n",
    "\n",
    "# Step 4: Concatenate heads\n",
    "attn_output_enc_dec = attn_output_enc_dec.transpose(1, 2).contiguous().view(1, tgt_tensor.size(1), embedding_dim)  # Shape: (1, tgt_seq_length, embedding_dim)\n",
    "\n",
    "# Step 5: Apply output linear layer\n",
    "attn_output_enc_dec = out_linear(attn_output_enc_dec)  # Shape: (1, tgt_seq_length, embedding_dim)\n",
    "\n",
    "# Step 6: Add residual connection and apply layer normalization\n",
    "tgt_embeddings = layer_norm1(tgt_embeddings + attn_output_enc_dec)  # Shape: (1, tgt_seq_length, embedding_dim)\n",
    "\n",
    "# =========================\n",
    "# Feed Forward Network\n",
    "# =========================\n",
    "\n",
    "# Step 1: Apply first linear layer\n",
    "ff_output = ff_linear1(tgt_embeddings)  # Shape: (1, tgt_seq_length, 64)\n",
    "\n",
    "# Step 2: Apply ReLU activation\n",
    "ff_output = ff_relu(ff_output)  # Shape: (1, tgt_seq_length, 64)\n",
    "\n",
    "# Step 3: Apply second linear layer\n",
    "ff_output = ff_linear2(ff_output)  # Shape: (1, tgt_seq_length, embedding_dim)\n",
    "\n",
    "# Step 4: Apply dropout\n",
    "ff_output = ff_dropout(ff_output)  # Shape: (1, tgt_seq_length, embedding_dim)\n",
    "\n",
    "# Step 5: Add residual connection and apply layer normalization\n",
    "tgt_embeddings = layer_norm2(tgt_embeddings + ff_output)  # Shape: (1, tgt_seq_length, embedding_dim)\n",
    "\n",
    "# =========================\n",
    "# Final Output Layer\n",
    "# =========================\n",
    "\n",
    "# Initialize final output linear layer\n",
    "fc_out = nn.Linear(embedding_dim, vocab_size_decoder + 1)  # +1 for padding index '0'\n",
    "\n",
    "# Step 1: Apply final linear layer to get logits\n",
    "output_logits = fc_out(tgt_embeddings)  # Shape: (1, tgt_seq_length, vocab_size_decoder + 1)\n",
    "\n",
    "# Step 2: Prevent the model from predicting the padding index '0' by setting its logit to -inf\n",
    "output_logits[:, :, 0] = -float('inf')\n",
    "\n",
    "# Step 3: Apply softmax to get probabilities\n",
    "output_probs = F.softmax(output_logits, dim=-1)  # Shape: (1, tgt_seq_length, vocab_size_decoder + 1)\n",
    "\n",
    "# Step 4: Get predicted token IDs by taking the argmax\n",
    "predicted_ids = torch.argmax(output_probs, dim=-1)  # Shape: (1, tgt_seq_length)\n",
    "print(\"Predicted Token IDs:\", predicted_ids)\n",
    "\n",
    "# Step 5: Convert predicted token IDs back to words\n",
    "predicted_sentence = numbers_to_sentence_decoder(predicted_ids.squeeze().tolist())\n",
    "print(\"Predicted Sentence:\", predicted_sentence)\n",
    "\n",
    "# =========================\n",
    "# Optimizer and Loss Calculation\n",
    "# =========================\n",
    "\n",
    "def train_model(model, loss_fn, train_data, val_data, max_iters, eval_interval):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "    for iter in range(max_iters):\n",
    "        # sample a batch of data\n",
    "        xb, yb = get_batch('train')\n",
    "\n",
    "        # evaluate the loss\n",
    "        logits, loss = model(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "# Calculate loss using cross-entropy\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding index '0'\n",
    "loss = loss_fn(output_logits.view(-1, vocab_size_decoder + 1), tgt_tensor.view(-1))\n",
    "print(\"Loss:\", loss.item())\n",
    "\n",
    "# =========================\n",
    "# Using the System in the Future\n",
    "# =========================\n",
    "\n",
    "def generate_translation(input_sentence, max_output_length, trained_model):\n",
    "    # Convert input sentence to numerical representation\n",
    "    encoded_input = sentence_to_numbers_encoder(input_sentence)\n",
    "    source_tensor = torch.tensor(encoded_input).unsqueeze(0)\n",
    "    \n",
    "    # Initialize output sequence with a start token\n",
    "    output_sequence = torch.zeros((1, 1), dtype=torch.long)\n",
    "    for _ in range(max_output_length):\n",
    "        # Use only the last max_output_length tokens as context\n",
    "        current_output = output_sequence[:, -max_output_length:]\n",
    "        # Generate logits and loss (unused) from the model\n",
    "        token_logits, _ = trained_model(current_output, source_tensor)\n",
    "        # Select logits for the most recent token\n",
    "        latest_token_logits = token_logits[:, -1, :]  # Shape: (batch_size, vocab_size)\n",
    "        # Convert logits to probabilities\n",
    "        token_probabilities = F.softmax(latest_token_logits, dim=-1)  # Shape: (batch_size, vocab_size)\n",
    "        # Randomly sample next token based on probabilities\n",
    "        next_token = torch.multinomial(token_probabilities, num_samples=1)  # Shape: (batch_size, 1)\n",
    "        # Append new token to the sequence\n",
    "        output_sequence = torch.cat((output_sequence, next_token), dim=1)  # Shape: (batch_size, sequence_length + 1)\n",
    "    return output_sequence\n",
    "\n",
    "\n",
    "# Example usage\n",
    "input_sentence = \"The dog jumped over the fence\"\n",
    "seq_length = 10\n",
    "model = create_transformer_model(vocab_size_encoder, vocab_size_decoder)\n",
    "# trained_model = train_model(model, loss_fn, train_data, val_data, max_iters, eval_interval)\n",
    "translated_sentence = generate_translation(input_sentence, seq_length, model)\n",
    "print(f\"Input: {input_sentence}\")\n",
    "print(f\"Translation: {translated_sentence}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f622bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
